---
layout: post
title:  "딥러닝 소개 2"
date:   2025-01-17T14:08:52+09:00
author: DINHO
categories:
  - 딥러닝-기초
  - 인공지능-분야-공부
cover:  "/assets/post/딥러닝.png"
---

제가 여름 방학 때 공부하면서 딥러닝에 대한 포스팅을 하기 위해 [딥러닝 소개](https://dinhoitt.github.io/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EC%B4%88/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%B6%84%EC%95%BC-%EA%B3%B5%EB%B6%80/2024/07/31/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%86%8C%EA%B0%9C.html) 를 포스팅했었는데요.
그 뒤로 공부만 하고 포스팅을 하지 못 했습니다...😅😥 이모저모 바쁘게 8월을 보내고 바로 학기가 시작하는 바람에 이렇게 되어 버렸네요... 이번 겨울 방학에는 무조건 이 시리즈를 마무리하겠습니다.🤩🤩(☞ﾟヮﾟ)☞(。・∀・)ノ 

그럼 지난 포스트에 이어서 딥러닝 소개 2탄 시작하겠습니다.

# Building Neural Networks with Perceptrons

지난 포스트에서 신경망들이 어떻게 작동되는지 살펴보겠다고 했는데요!! 먼저 뉴런(퍼셉트론)을 간단하게 표현해보겠습니다.

<div align="center">
	<img src="/assets/post/퍼셉트론2.png" alt="Simplifed Perceptron">
	<p><em>[그림 1] Simplifed Perceptron</em></p>
</div>

이렇게 간단하게 해도 결국 이것만 기억하시면 됩니다. 입력과 가중치를 내적하고 바이어스를 더해서 비선형활성화함수를 통과시킨다!! 간단화한 그림은 출력 y가 g(z)로 간단하게 표현되고 있습니다. 

그렇다면 이제 출력이 두 개이면 어떻게 될까요?

<div align="center">
	<img src="/assets/post/퍼셉트론3.png" alt="Multi OutPut Perceptron">
	<p><em>[그림 2] Multi OutPut Perceptron</em></p>
</div>

각 뉴런은 하나의 답을 예측하게 될 겁니다. 이렇게 다중 뉴런에서 가장 중요한 것은 각 뉴런마다 고유한 가중치를 가진다는 것입니다. 이들은 독립적으로 소통하지만 나중에 다른 층이 추가되면 서로 소통할 수 있습니다. 모든 입력이 모든 출력과 densly하게 연결되어 있어서 이러한 레이어들을 Dense layers라고 합니다.

이 과정을 프로그래밍적으로 생각해보겠습니다. 수식들이 어렵지 않았으니 비교적 간단하게 구현되겠죠?

```Python
# tensorflow사용 예제
import tensorflow as tf
class MyDenseLayer(tf.keras.layers.Layer):
	def __init__(self, input_dim, output_dim):
		super(MyDenseLayer, self).__init__()

		# Initialize weights and bias
		self.w = self.add_weight([input_dim, output_dim])
		self.b = self.add_weight([1, output_dim])

	#실제로 정보가 레이어를 통해서 어떻게 전달 되는지 보기 위해 호출 함수 생성 
	def call(self, inputs):
		# Forward propagate the inputs
		z = tf.matmul(inputs, self.w) + self.b

		# Feed through a non-linear activation
		output = tf.math.sigmoid(z)

		return output

# pytorch사용 예제
import torch
import torch.nn as nn
class MyDenseLayer(nn.module):
	def __init__(self, input_dim, output_dim):
		super(MyDenseLayer, self).__init__()

		# Initialize weights and bias
		self.w = nn.Parameter(torch.radn(input_dim, output_dim, requires_grad=True))
		self.b = nn.Parameter(torch.radn(1, output_dim, requires_grad=True))

	#실제로 정보가 레이어를 통해서 어떻게 전달 되는지 보기 위해 호출 함수 생성 
	def call(self, inputs):
		# Forward propagate the inputs
		z = tf.matmul(inputs, self.w) + self.b

		# Feed through a non-linear activation
		output = tf.math.sigmoid(z)
		return output
```

하지만 최근 딥러닝 툴박스와 라이브러리들이 이미 많은 것을 구현해주기 때문에 한 줄로 딸깍 할 수 있습니다.

```Python
# tensorflow
import tensorflow as tf

layer = tf.keras.layers.Dense(units=2)

# pytorch
import torch
import torch.nn as nn

layer = nn.Linear(in_features=m,out_features=2)
```

신경망의 복잡성을 천천히 높여보겠습니다.

여기 Single Layer Neural Network를 보겠습니다.

<div align="center">
	<img src="/assets/post/퍼셉트론4.png" alt="Single Layer Neural Network">
	<p><em>[그림 3] Single Layer Neural Network</em></p>
</div>

가운데에 있는 레이어는 히든 레이어(Hidden Layer)라고 합니다. 직접적으로 관찰하거나 감독할 수 없어서 그렇게 부릅니다. 입력과 출력은 직접 관찰할 수 있지만, 히든레이어는 볼 수 없습니다.
아까 그림과는 다르게, 이제는 입력에서 히든 레이어로, 히든 레이어에서 출력으로 변환하는 함수가 있으니 레이어가 2개인 신경망을 가지게 됐습니다. 즉, 두 개의 가중치 행렬을 가지게 됩니다.

<div align="center">
	<img src="/assets/post/퍼셉트론5.png" alt="Single Layer Neural Network">
	<p><em>[그림 4] Single Layer Neural Network</em></p>
</div>

히든레이어의 단일 유닛을 한 번 보겠습니다. Z2를 예로 보겠습니다. 이 뉴런의 값을 구하는 것은 계속 이야기 한 것과 동일합니다.
가중치와 입력값의 내적을 구하고, 바이어스를 더한 수 비선형활성화함수를 적용하면 됩니다. Z3, Z4도 마찬가지입니다. 가중치 값만 다를 뿐, 답을 구하는 방법은 같습니다.

<div align="center">
	<img src="/assets/post/퍼셉트론6.png" alt="Multi OutPut Perceptron">
	<p><em>[그림 5] Multi OutPut Perceptron</em></p>
</div>

이제 그림을 좀 단순화 해서 모든 연결 선을 박스 기호로 대체하겠습니다. 이 기호는 Fully Connected Layer를 뜻합니다. 
아까 코드를 잠깐 봤으니 이 그림도 간단하게 코드로 구현할 수 있습니다.

```Python
# Tensorflow
import tensorflow as tf

model = tf.keras.Sequential([
	tf.keras.layers.Dense(n),
	tf.keras.layers.Dense(2)
])

# Pytorch
from torch import nn

model = nn.Sequential(
	nn.Linear(m,n),
	nn.ReLU(),
	nn.Linear(n,2)
)
```

이제는 이전과 달리 단순한 신경망이 아니라 더 깊어진 신경망을 보겠습니다.

<div align="center">
	<img src="/assets/post/dnn.png" alt="Deep Neural Network">
	<p><em>[그림 6] Deep Neural Network</em></p>
</div>

레이어들을 하나씩 쌓아가며 점점 더 계층적인 모델을 만들 수 있습니다. 최종 출력은 신경망을 점점 더 깊게 깊게 통과하면서 계산됩니다.

코드로 구현할 때도 같습니다. 코드 내부에서 층들을 차례로 쌓아가면서 네트워크를 깊게 만들면 됩니다.

이러한 기초 지식들로 신경망을 적용하는 방법에 대해 이야기해보겠습니다.

# Applying Neural Network

가장 유명한 예시를 들어보겠습니다. 

### Will I pass this class?

간단한 두 feature 모델과 함께 시작하겠습니다. $$x_{1}$$ 은 강의 출석 수이고 $$x_{2}$$ 는 최종 프로젝트에 투자한 히간이라 하겠습니다.

<div align="center">
	<img src="/assets/post/dnnex1.png" alt="Example Problem: Will I pass this class?">
	<p><em>[그림 7] Example Problem: Will I pass this class?</em></p>
</div>

몇 년 간의 수업 관찬을 토대로 위와 같은 데이터가 있다고 합시다. 한 점은 사람을 나타내고 ,색깔은 수업 통과 여부를 나타냅니다. 

<div align="center">
	<img src="/assets/post/dnnex2.png" alt="Example Problem: Will I pass this class?">
	<p><em>[그림 8] Example Problem: Will I pass this class?</em></p>
</div>

제가 보라색이라고 해보겠습니다. 이 피처 공간에 [4,5] 지점이 있다고 해보겠습니다. 이제 신경망을 구축하고, 이전 수업들의 모든 학생들의 정보를 이용해 신경망이 제가 수업을 통과할 가능성에 대해 예측하는 겁니다.

<div align="center">
	<img src="/assets/post/dnnex3.png" alt="Example Problem: Will I pass this class?">
	<p><em>[그림 9] Example Problem: Will I pass this class?</em></p>
</div>

현재 두 개의 입력 값이 있습니다. 이 값들은 단일 히든레이어 신경망에 입력됩니다. 이 때 예측된 확률은 10%라는 것을 알 수 있습니다. 하지만 위에 그림을 봤었을 때 비교하면 타당해 보이나요?? 그렇지 않습니다. 수업 5 번 중 4 번을 참여했고 프로젝트에 5시간을 투자하였으니 실제로는 수업에서 통과할 겁니다. 왜 이런 터무니 없는 예측을 했을까요? 아직 위 그림의 데이터를 보여주지 않았기 때문읍니다. 즉, 신경망 훈련이 안 됐기 때문입니다. 우리는 이 신경망을 훈련시켜야 합니다. 이제 신경망을 훈련시켜보겠습니다.

<div align="center">
	<img src="/assets/post/dnnex4.png" alt="Quantifying Loss">
	<p><em>[그림 10] Quantifying Loss</em></p>
</div>

먼저 신경망을 훈련시키려면 신경망이 잘못된 결정을 내릴 때마다 알려줘야 합니다. 우리는 이것을 Loss(손실)라고 합니다. 실제 값과 예측 값의 차이를 계산하고, Loss를 줄이는 방식으로 학습합니다. Loss 함수에는 여러 종류가 있습니다. L1, L2 거리, Cross-Entropy 등등 다양한 Loss 종류들이 있습니다.

<div align="center">
	<img src="/assets/post/dnnex5.png" alt="Binary Cross Entropy Loss">
	<p><em>[그림 11] Binary Cross Entropy Loss</em></p>
</div>

한 학생뿐 아니라 여러 학생들의 데이터로 학습을 하고 Loss를 줄이며 신경망은 학습하게 됩니다. 예측과 실제 값이 가까울수록 손실은 작아져야 하고, 모델의 정확도는 높아져야 합니다.

이 신경망에게 정답을 알려주기 위해 그림처럼 Softmax(Softmax Cross-Entropy)를 사용할 수 있습니다. Softmax는 신경망에게 두 확률 분포가 얼마나 떨어져 있는지 알려주는 손실함수입니다. 그래서 출력은 확률 분포가 되고, 신경망이 더 나은 답을 내도록 피드백할 수 있게 됩니다.

<div align="center">
	<img src="/assets/post/dnnex6.png" alt="Mean Squared Error Loss">
	<p><em>[그림 12] Mean Squared Error Loss</em></p>
</div>

이제 이진 출력을 예측하거나 훈련하는 대신, 임의의 숫자를 예측하고 싶으면 어떡할까요? 그 숫자는 양의 무한대부터 음의 무한대까지 어떤 숫자든 될 수 있습니다. 즉 제가 받을 학점을 예측하고 싶다면 그 학점이 꼭 0에서 1이거나 0에서 100일 필요는 없습니다.

MSE(Mean Squared Error)라는 손실을 사용해서 그 값을 만들 수 있습니다. 이 경우에는 확률분포가 아닌 수업에서 받은 진짜 학점과 예측된 학점 사이의 MSE를 계산 할 수 있겠네요. 부호는 상관 없이, 제곱하여 거리를 계산한 값이 MSE입니다. 결국 신경망은 Loss를 최소화 하여 입력값에 대한 출력을 예측하는 것입니다.

# Training Neural Networks

그렇다면 손실 정보와 네트워크 가중치를 찾는 문제를 하나로 합쳐보겠습니다. 우리는 결국 이 방대한 데이터에 대해 평균적으로 문제를 해결할 수 있는 신경망을 찾고 있는 겁니다.

$$
W^* = \underset{W}{\arg\min} \frac{1}{n} \sum_{i=1}^n \mathcal{L}(f(x^{(i)}; W), y^{(i)})

W^* = \underset{W}{\arg\min} J(W)
$$

즉 우리가 찾는 것은 신경망의 가중치가 무엇인지입니다. 계속 예기한 벡터 W를 찾고 싶습니다. 모든 데이터를 바탕으로 이 벡터 W를 계산할 수 있습니다. 그리고 이 벡터 W는 손실이 어떻게 될지도 결정합니다. 손실은 뭐였죠? 신경망이 예측한 값과 실제 값에서 얼마나 벗어났는지 나타내던 거였습니다. 

이 가중치 벡터는 숫자들의 모음이라는 것을 잘 생각해봅시다. 그래서 많은 데이터를 바탕으로 얻은 이 벡터는 무엇일까요? 그것이 신경망을 학습해야 할 문제입니다.

<div align="center">
	<img src="/assets/post/lossopt.png" alt="Loss Optimization">
	<p><em>[그림 13] Loss Optimization</em></p>
</div>

이전에 봤던 것처럼 신경망에 가중치가 두 개밖에 없다면, 그림과 같이 지형도를 그릴 수 있습니다. 이 모든 가중치 조합들에 대해서 손실은 특정 값을 찾게 될 텐데, 그래프에서는 z축이 되겠네요. 이 그림을 통해 두 가중치에 대해 손실이 얼마인지 봅시다.

<div align="center">
	<img src="/assets/post/lossopt2.png" alt="Loss Optimization">
	<p><em>[그림 14] Loss Optimization</em></p>
</div>

결국 가장 찾고 싶은 건 손실이 가장 낮은 지점입니다. 최적의 손실이죠. 그래프의 아무 지점에서 시작합니다. 그 지점에서 gradient, 즉 기울기 $$ \frac{\partial J(W)}{\partial W}$$ 를 계산합니다. 그 특정 지점에서 어느 방향이 기울없는지를 아주 국소적으로 추정하는 겁니다. 경사가 어디로 증가하는지, 혹은 감소하는지를 계산하고, landscape를 따라 내려가면 최소 지점을 찾을 수 있겠죠.

<div align="center">
	<img src="/assets/post/lossopt3.png" alt="Loss Optimization">
	<p><em>[그림 15] Loss Optimization</em></p>
</div>